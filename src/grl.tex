\chapter{Graph Representation Learning}\label{grl}
Here we give an insight on some techniques used to learn the underlying structure of graphs. Specifically, in \Cref{sec:graphLearningProblems} we introduce the main machine learning tasks involving graph data. In \Cref{sec:shallowEmbeddings} we delve into node embeddings, a fundamental concept in graph representation learning and we show how these embeddings can be adapted for knowledge graphs in \Cref{sec:embeddingsKGs}. Finally we discuss more recent methods that allow to transpose the learned representation to new nodes (\Cref{sec:gnn}).

\section{Graph Learning Problems}\label{sec:graphLearningProblems}
Machine learning is fundamentally a problem-driven discipline focused on developing models that learn from data to address specific tasks. These machine learning models typically depend the type of task they aim to solve: supervised or unsupervised. This principle is equally applicable to learning with graphs; however, the conventional distinctions of supervised and unsupervised learning may not always provide the most informative or useful framework when dealing with graph data. In this section, we provide a concise overview of machine learning tasks involving graph data. As will be illustrated, machine learning challenges concerning graphs frequently blur the lines between traditional machine learning categories.

\subsection{Node Classification}
Given a graph $ G = (V,E,L) $, the objective of \term{node classification} is to predict the label associated with each node $ v \in V $ a training set of nodes $ V_\text{train} \subset V $ containing true labels. It is common to assume that label information is available for only a limited subset of nodes, although there are instances where a considerable number of nodes are labeled, such as in the case of classifying the functions of proteins within the interactomes of various species~\cite{Hamilton2017inductiveRepresentationLearning}.

The nodes in a graph are not independent and identically distributed; rather, they constitute an interconnected set. The central insight behind many successful node classification methods is the explicit utilization of connections among nodes. A particularly popular approach is to exploit \term{homophily}, the tendency of nodes to share attributes with their neighbours in the graph~\cite{Mcpherson2001homophilyInSocialNw}. For instance, it is common for individuals to form friendships with others who have similar interests or demographics. Based on the principle of homophily, one can develop machine learning models that assign similar labels to neighbouring nodes in a graph~\cite{Zhou2003LearningLocalGlobalConsistency}. In addition to homophily, concepts such as structural equivalence~\cite{Donnat2017GraphWaveletsStructuralRoleSimilarityComplexNws}, which postulates that nodes with similar local neighbourhood structures will have analogous labels, and \term{heterophily}, which suggests that nodes are preferentially connected to nodes with differing labels, also play a role. When constructing node classification models, it is essential to leverage these concepts instead of treating nodes as independent data points like one would do in standard machine learning.

\subsection{Link Prediction}
The standard framework for \term{link prediction} involves a set of nodes $ V $ and an incomplete set of edges connecting these nodes $ E_\text{train} \subset E $. The goal is to utilize this partial information to infer the missing edges $ E \setminus E_\text{train} $. Link prediction is one of the most popular machine learning tasks applied to graph data and has a multitude of applications, including predicting drug side effects~\cite{Zitnik2018ModelingPolypharmacySideEffectsGCN}. In complex multi-relational graph datasets, such as Biological Knowledge Graphs (BKGs) encoding numerous distinct biological interactions, link prediction may call for sophisticated reasoning and inference strategies~\cite{Nickel2015ReviewRelationalMLKG}.

\subsection{Clustering and Community Detection}
Both node classification and link prediction infer missing information about graph data, essentially positioning these tasks as graph analogs of supervised learning. In contrast, community detection represents the equivalent of unsupervised clustering in the graph context. The fundamental intuition behind clustering is that the graph exhibits a community structure, wherein nodes are significantly more likely to form edges with nodes that are part of the same community. The challenge lies in inferring latent community structures based solely on the input graph $ G = (V, E) $. Numerous real-world applications of community detection include uncovering functional modules in genetic interaction networks~\cite{Agrawal2018LargeScaleAnalysisDiseasePathwaysHumanInteractome}.

\subsection{Why Graph Representation Learning?}
To address the aforementioned challenges, various ``traditional'' methods exist~\cite{Hamilton2020GraphRL}, including:
\begin{itemize}
    \item graph statistics and kernel methods for extracting feature information relevant for classification tasks,
    \item neighbourhood overlap detection as a heuristic for link prediction,
    \item spectral clustering and graph Laplacians to cluster nodes into communities.
\end{itemize}

These methodologies require carefully crafted, hand-engineered statistics and measures that lack adaptability through a learning process, and the design of these features can be both time-consuming and costly. The following sections propose an alternative approach to graph analysis: graph representation learning. Instead of extracting predetermined features, we will focus on learning representations that encapsulate structural information about the graph.

\section{Node Embeddings}\label{sec:shallowEmbeddings}
The aim of node embedding methodologies is to encode nodes into low-dimensional vectors, or \term{embeddings}, which encapsulate their position within the graph and the structure of their local neighbourhoods. In essence, we aspire to project nodes into a latent space where geometric relationships within this space align with relationships in the original graph~\cite{Hoff2002latentSpaceApproachesSocialNetworkAnalysis}. In \Cref{fig:nodeEmbedding}, we illustrate this problem, where the objective is to learn an encoder that maps nodes to a low-dimensional embedding space. These embeddings are optimized such that distances in the embedding space accurately reflect the relative positions of nodes in the original graph.

\input{figs/figNodeEmbedding}
We introduce an encoder-decoder framework to illustrate the workings of node embeddings and subsequently we describe several methodologies for node embedding, with a focus on random walk approaches utilized as baselines in our experimental framework.

\subsection{An Encoder-Decoder Framework}
Our discussion of node embeddings is organized around the framework of encoding and decoding graphs, closely mirroring the itinerary followed in~\cite{Hamilton2020GraphRL}. Within the encoder-decoder framework, the problem of graph representation learning involves two primary operations:
\begin{description}
    \item[encoder] maps each node in the graph to a low-dimensional embedding,
    \item[decoder] utilizes the low-dimensional node embeddings to reconstruct information about each node's neighbourhood in the original graph.
\end{description}
This concept is summarized in \Cref{fig:encDecFramework}. In this section, we focus on \term{shallow} embedding methods. Generalized encoder architectures that transcend shallow embeddings are referred to as graph neural networks (GNNs)and are the subject of discussion in the subsequent section.

\input{figs/figEncDecFramework}

\subsubsection{Encoder}
Formally, an \term{encoder} is defined as a function that maps nodes to vector embeddings. In its simplest form, the encoder has the following signature:
\begin{equation*}
    \text{ENC}: V \to \mathbb{R}^d.
\end{equation*}
In the context of \term{shallow} node embeddings, the encoder function operates as an embedding lookup based on the node ID. For a node $ v\in V $, we have:
\begin{equation}\label{eq:encoderLookup}
    \text{ENC}(v) = \mathbf{z}[v]
\end{equation}
where $ \mathbf{z}\in \mathbb{R}^{V\times d} $ denotes a matrix containing the embedding vectors for all nodes, and $ \mathbb{Z}[v] $ indicates the row of the matrix corresponding to node $ v $.

\subsubsection{Decoder}\label{sec:decoder}
The role of the \term{decoder} is to reconstruct certain graph statistics from the node embeddings generated by the encoder. For instance, given the node embedding $ \mathbf{z}_v $ of a node $ v $, the decoder may attempt to predict its neighbourhood $ \mathcal{N}(v) $. The most common definition is that of a \term{pairwise} decoder, which has the following signature:
\begin{equation*}
    \text{DEC}: \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R}^+.
\end{equation*}
Pairwise decoders estimate relationships or similarities between pairs of nodes. 

Applying the pairwise decoder to a pair of embeddings $ (\mathbf{z}_u, \mathbf{z}_v) $ results in the \term{reconstruction} of the relationship between nodes $ u $ and $ v $. The objective is to optimize the encoder and decoder to minimize the reconstruction loss such that:
\begin{equation}\label{eq:reconstruction}
    \text{DEC}(\text{ENC}(u), \text{ENC}(v)) = \text{DEC}(\mathbf{z}_u, \mathbf{z}_v) \approx \textbf{S}(u,v)
\end{equation}
where $ \textbf{S} $ stands for a graph similarity measure between nodes. When predicting edges in a graph, the similarity function for a pair of nodes could correspond to the respective entry within the graph's adjacency matrix.

\subsubsection{Optimizing the Encoder-Decoder Model}
To achieve the reconstruction objective outlined in \Cref{eq:reconstruction}, it is standard practice to minimize an empirical reconstruction loss $ \mathcal{L} $ over a set of training node pairs $ \mathcal{D} $:
\begin{equation}\label{eq:loss}
    \mathcal{L} = \sum_{(u,v)\in\mathcal{D}} \ell(\text{DEC}(\mathbf{z}_u, \mathbf{z}_v), \textbf{S}[u,v])
\end{equation}
where $ \ell: \mathbb{R}\times\mathbb{R}\to\mathbb{R} $ denotes a loss function measuring the discrepancy between the decoded similarity values $ \text{DEC}(\mathbf{z}_u, \mathbf{z}_v) $ and the true values $ \textbf{S}[u,v] $. The choice of the loss function $ \ell $ depends on the chosen decoder and similarity measure $ \textbf{S} $. Common loss functions include mean-squared error for regression tasks and cross-entropy for classification tasks. Most approaches adopt stochastic gradient descent to minimize the loss presented in \Cref{eq:loss}~\cite{Robbins1951stochasticApproximation}. However, specialized optimization methods, such as those based on matrix factorization, can also be utilized due to their theoretical connections to spectral clustering.

We proceed by delineating several representative node embedding methods. We initially discuss node embedding methods justified by matrix factorization approaches. Then we explore more recent methods based on random walks, initially inspired by techniques from natural language processing. Our focus on these recent methods stems from their utility as baseline performance measures in our experiments.

\subsection{Factorization-Based Approaches}\label{sec:factorization}
One perspective on the encoder-decoder framework is through the lens of matrix factorization. Indeed, the challenge of decoding local neighbourhood structure from a node's embedding closely relates to reconstructing entries within the graph adjacency matrix. In a broader sense, we can consider this task in terms of employing matrix factorization to learn a low-dimensional approximation of a node-node similarity matrix $ \mathbf{S} $, where $ \mathbf{S} $ generalizes the adjacency matrix and captures user-defined notions of node-node similarity as addressed in \Cref{sec:decoder}~\cite{Belkin2001laplacianEigenmapsSpectralClusteringTechniquesEmbeddingClustering}\cite{Kruskal1964MultidimensionalScalingOptimizingGoodnessFitNonmetricHypothesis}.

\subsubsection{Laplacian Eigenmaps}
Among the earliest factorization-based approaches is the \term{Laplacian eigenmaps} (\term{LE}) technique, which is built on spectral clustering~\cite{Belkin2001laplacianEigenmapsSpectralClusteringTechniquesEmbeddingClustering}. In this method, the decoder is defined based on the $ L_2 $ distance between node embeddings:
\begin{equation*}
    \text{DEC}(\mathbf{z}_u, \mathbf{z}_v) = \|\mathbf{z}_u - \mathbf{z}_v\|_2^2.
\end{equation*}
The loss function weighs pairs of nodes according to their similarity within the graph:
\begin{equation}\label{eq:laplacianEigenmaploss}
    \mathcal{L} = \sum_{(u,v)\in\mathcal{D}} \text{DEC}(\mathbf{z}_u, \mathbf{z}_v) \cdot \textbf{S}[u,v].
\end{equation}

The underlying intuition for this approach is that \Cref{eq:laplacianEigenmaploss} penalizes the model when highly similar nodes have embeddings that are distant from one another. When $ \mathbf{S} $ is structured like a Laplacian matrix and if the embeddings $ z_u $ are $ d $-dimensional, then the node embeddings minimizing the loss in \Cref{eq:laplacianEigenmaploss} correspond to the $ d $ smallest eigenvectors of the Laplacian (excluding the eigenvector comprising all ones).

\subsubsection{Inner-Product Methods}
More recent work commonly employs an inner-product based decoder defined as follows:
\begin{equation}\label{eq:innerProductDec}
    \text{DEC}(\mathbf{z}_u, \mathbf{z}_v) = \mathbf{z}_u^T\mathbf{z}_v.
\end{equation}
In this case, the assumption is that the similarity between two nodes is proportional to the dot product of their embeddings. These embedding methodologies~\cite{Ahmed2013distributedLargeScaleNaturalGraphFactorization}\cite{Cao2015grarep}\cite{Ou2016asymmetricTransitivityPreservingGraphEmbedding} integrate the inner-product decoder with a mean-squared error loss function:
\begin{equation*}
    \mathcal{L} = \sum_{(u,v)\in\mathcal{D}} \|\text{DEC}(\mathbf{z}_u, \mathbf{z}_v) - \textbf{S}[u,v]\|_2^2.
\end{equation*}
These methods are categorized as matrix-factorization approaches, as their loss functions can be minimized through factorization algorithms like \term{singular value decomposition} (\term{SVD}). By stacking the node embeddings $ \mathbf{z}_u \in \mathbb{R}^d $ into a matrix $ \mathbf{Z} \in \mathbb{R}^{|V|\times d} $, the reconstruction objective can be expressed as:
\begin{equation*}
    \mathcal{L} \approx \|\mathbf{Z}\mathbf{Z}^\top - \textbf{S}\|_2^2,
\end{equation*}
which represents a low-dimensional factorization of the node-node similarity matrix $ \textbf{S} $. 

\subsection{Random Walk Embeddings}
In these approaches the node embeddings are optimized such that two nodes will have comparable embeddings if they frequently co-occur on short random walks throughout the graph.

\subsubsection{DeepWalk \& node2vec}
Similar to the aforementioned matrix factorization approaches, both \textit{DeepWalk}~\cite{Perozzi2014DeepWalk} and \textit{node2vec}~\cite{Grover2016node2vec} adopt a shallow embedding approach alongside an inner-product decoder.

The difference between these methods lies in their definitions of node similarity and neighbourhood reconstruction. Instead of directly reconstructing the adjacency matrix $ \mathbf{A} $ (or a deterministic function of $ \mathbf{A} $), these methods optimize embeddings to encode the statistics associated with random walks. Mathematically, the objective is to learn embeddings such that the following approximately holds:
\begin{equation}\label{eq:randomWalkdec}
    \text{DEC}(\mathbf{z}_u, \mathbf{z}_v) = \frac{e^{\mathbf{z}_u^\top\mathbf{z}_v}}{\sum_{v_k\in V} e^{\mathbf{z}_u^\top\mathbf{z}_k}} \approx p_{G,T}(v|u),
\end{equation}
where $p_{G,T}(v|u)$, approximated with a softmax function, is the probability of visiting $v$ on a length-$T$ random walk starting at $u$, with $T$ usually defined to be in the range $T\in\{2,...,10\}$. Again, a key difference between this equation and the factorization-based approaches is that the similarity measure is both stochastic and asymmetric.
To train random walk embeddings, the general strategy is to use the decoder from \Cref{eq:randomWalkdec} and minimize the following cross-entropy loss:
\begin{equation}\label{eq:randomWalkloss}
    \mathcal{L} = \sum_{(u,v)\in\mathcal{D}} -\log(\text{DEC}(\mathbf{z}_u, \mathbf{z}_v)).
\end{equation}

Here, we use $\mathcal{D}$ to refer to a training set of random walks, which is generated by sampling random walks starting from each node.
Evaluating the loss in \Cref{eq:randomWalkloss} can be computationally intensive. Notably, the evaluation of the denominator in \Cref{eq:randomWalkdec} alone has a time complexity of $ O(|V|) $, rendering the overall time complexity for loss function evaluation $ O(|\mathcal{D}||V|) $. Various strategies exist to surmount this computational challenge, marking a fundamental difference between the original \textit{DeepWalk} and \textit{node2vec} algorithms. The \textit{DeepWalk} algorithm employs a \term{hierarchical softmax}\footnote{Hierarchical softmax approximates softmax efficiently by structuring output classes (graph nodes) into a binary tree, reducing complexity from $ O(N) $ to $ O(\log(N)) $. Probabilities are computed along the path from root to leaf using logistic sigmoid functions, with a Huffman tree positioning frequent classes closer to the root.} to approximate \Cref{eq:randomWalkdec}. In contrast, \textit{node2vec} applies a noise contrastive approach to approximate \Cref{eq:randomWalkloss}, wherein the normalizing factor is estimated using negative samples.

A noteworthy distinction of the \textit{node2vec} approach is its provision for a more flexible definition of random walks. While \textit{DeepWalk} utilizes uniformly random walks to define $ p_{G,T}(v|u) $, the \textit{node2vec} approach leverages second order random walks, the next state depends not only on the current state but also on the previous state. This results in hyperparameters that bias the walk to either favor a depth-first or breadth-first search.

\subsubsection{Large-Scale Information Network Embeddings (LINE)}
The method \textit{LINE}~\cite{Tang2015line} does not explicitly harness random walk strategies, but shares conceptual underpinnings with both \textit{DeepWalk} and \textit{node2vec}. The primary concept in LINE is to combine two encoder-decoder objectives. The first objective encodes first-order adjacency information, employing the following decoder:
\begin{equation*}
    \text{DEC}(\mathbf{z}_u, \mathbf{z}_v) = \frac{1}{1 + e^{-\mathbf{z}_u^\top\mathbf{z}_v}}.
\end{equation*}
This decoder relies on an adjacency-based similarity measure (i.e., $ \textbf{S}[u,v] = \mathbf{A}[u,v] $). The second objective mirrors random walk approaches, using the same decoder as in \Cref{eq:randomWalkdec}, but trained using Kullback-Leibler divergence~\footnote{Kullback–Leibler (KL) divergence measures the statistical distance between two probability distributions $P$ and $Q$, defined as $ D_{\text{KL}}(P\parallel Q)=\sum_{x\in \mathcal{X}}P(x)\log \left(\frac{P(x)}{Q(x)}\right) $, with $\mathcal{X}$ being the set of possible observations.} to encode two-hop adjacency information (i.e., the data encapsulated in $ \mathbf{A}^2 $). Therefore, LINE is conceptually tied to \textit{node2vec} and \textit{DeepWalk}, yet it reconstructs first- and second-order neighbourhood information without sampling random walks.

\subsection{Limitations of Shallow Embeddings}
Shallow embedding approaches define the encoder model simply as an embedding lookup (\Cref{eq:encoderLookup}), which trains a unique embedding for each node within the graph. These approaches exhibit several notable drawbacks:
\begin{itemize}
    \item They do not share any parameters between nodes in the encoder, since the encoder directly optimizes a unique embedding vector for each node. This absence of parameter sharing is inefficient from both statistical and computational standpoints. Statistically, parameter sharing can serve as a powerful regularization method. Computationally, the lack of parameter sharing results in a growing number of parameters as $ O(V) $.
    \item They do not exploit node features within the encoder. Many graph datasets are rich in feature information, which could prove informative during the encoding process.
    \item These methods are inherently \term{transductive}, meaning they can only generate embeddings for nodes present during the training phase. Generating embeddings for new nodes is unfeasible unless additional optimizations are conducted. This limitation inhibits the use of shallow embedding methods in \term{inductive} applications, which entail generalization to unseen nodes post-training.
\end{itemize}
To mitigate these limitations, shallow encoders may be replaces with more sophisticated counterparts that generally rely on the structure and attributes of the graph. In \Cref{sec:gnn}, we will discuss the most prevalent paradigm for defining such encoders: graph neural networks (GNNs).

\section{Embeddings for Knowledge Graphs (KGs)}\label{sec:embeddingsKGs}
Maintaining our focus on shallow embedding methods, we now pivot towards knowledge graphs, specifically directed edge-labeled graphs.

\paragraph{Knowledge Graph Completion}
The methods presented in this section were primarily designed for the task of knowledge graph completion. Given a directed edge-labeled graph $ G = (V, E, L) $, a generic edge is denoted as $ (u,\ell,v) $, indicating the presence of a particular relation $ \ell \in L $ between nodes $ u $ and $ v $. An edge label can be interpreted as a fact that holds between two nodes. The overarching goal of knowledge graph completion is to predict missing edges within the graph.

\subsection{Reconstructing Labeled Data}
Similarly to the simple graphs explored in \Cref{sec:shallowEmbeddings}, one can view the embedding of knowledge graphs as a reconstruction task. Given the embeddings $ \mathbf{z}_u $ and $ \mathbf{z}_v $ of two nodes, the objective is to reconstruct the relationship between these nodes while addressing the presence of multiple distinct edge types. To accommodate this complexity, we augment our decoder to function in a multi-relational manner. Consequently, the decoder is defined to accept not merely a pair of node embeddings as input, but also their relation type:
\begin{equation*}
    \text{DEC}: \mathbb{R}^d \times L \times \mathbb{R}^d \to \mathbb{R}^+.
\end{equation*}

The output of this decoder can be interpreted as the likelihood that the edge $ (u,\ell,v) $ exists within the graph.

One of the earliest and simplest approaches to learning knowledge graph embeddings is known as \textit{RESCAL}, which defines the decoder as:
\begin{equation}\label{eq:RESCAL}
    \text{DEC}(\mathbf{z}_u, \ell, \mathbf{z}_v) = \mathbf{z}_u^\top \mathbf{R}_\ell \mathbf{z}_v.
\end{equation}
In this expression, $ \mathbf{R}_\ell\in \mathbb{R}^{d\times d} $ represents a learnable matrix associated with the specific relation type $ \ell\in L $. The fundamental reconstruction loss function is then expressed as:
\begin{equation}\label{eq:reconstructionLossMulti}
    \mathcal{L} = \sum_{u\in V}\sum_{v\in V}\sum_{\ell\in L} \|\mathbf{z}_u^\top \mathbf{R}_\ell \mathbf{z}_v - \mathbf{A}[u,\ell,v]\|_2^2,
\end{equation}
where $ \mathbf{A}\in\mathbb{R}^{|V|\times|L|\times|V|} $ denotes the adjacency tensor for the knowledge graph, thereby generalizing the matrix factorization approaches discussed in \Cref{sec:factorization}.

\medskip
In \Cref{sec:shallowEmbeddings}, we highlighted how the diversity of methods for node embeddings largely arises from the choice of different decoders, similarity measures, and loss functions. In the context of knowledge graphs, nearly all multi-relational embedding methods directly define the similarity measure based on the adjacency tensor. This is primarily due to the challenges associated with establishing higher-order neighbourhood relationships in knowledge graphs, as well as the fact that most multi-relational embedding methods were uniquely designed for relation prediction.

\subsection{Loss Functions}
To account for the considered loss functions, we first address the shortcomings of the simple reconstruction loss introduced in \Cref{eq:reconstructionLossMulti}. The first issue is its computational expense: the nested summations in the equation require $ O(|V|^2|L|) $ operations, leading to prohibitive computation times in many large graphs. Additionally, since many knowledge graphs are sparse, (so $ |E| \ll |V|^2|L| $),a loss function with complexity reduced to $ O(|E|) $ would be preferable. The second issue is somewhat more nuanced; our aim is to decode the adjacency tensor using the low-dimensional node embeddings. Although this tensor generally contains only binary values, the mean-squared error discussed in \Cref{eq:reconstructionLossMulti} is not particularly suited for binary comparisons since it is aimed at regression tasks.

\subsubsection{Cross-Entropy with Negative Sampling}
A widely adopted loss function that is both efficient and appropriate for the task is the \term{cross-entropy loss with negative sampling}. We define this loss as:
\begin{equation*}
    \mathcal{L} = \sum_{(u,\ell,v)\in E} -\log(\sigma(\text{DEC}(\mathbf{z}_u, \ell, \mathbf{z}_v))) - \gamma\mathbb{E}_{v_n\sim P_{n,u}(V)}[\log(\sigma(-\text{DEC}(\mathbf{z}_u, \ell, \mathbf{z}_{v_n})))]
\end{equation*}

where $ \sigma $ indicates the logistic function, and $ P_{n,u}(V) $ denotes a ``negative sampling'' distribution over the set of nodes $ V $ ($|V|=n$) which may depend on $ u $, with $ \gamma > 0 $ as a hyperparameter. 
We classify this as a cross-entropy loss because it derives from the standard binary cross-entropy loss. By feeding the output of the decoder into a logistic function, we obtain normalized scores within the interval $[0,1]$, interpretable as probabilities. The term
\begin{equation}\label{eq:logLikelihood}
    \log(\sigma(\text{DEC}(\mathbf{z}_u, \ell, \mathbf{z}_v)))
\end{equation}
represents the log-likelihood that we predict ``true'' for an edge that indeed exists in the graph. Conversely, the term
\begin{equation}\label{eq:negativeLogLikelihood}
    \mathbb{E}_{v_n\sim P_{n,u}(V)}[\log(\sigma(-\text{DEC}(\mathbf{z}_u, \ell, \mathbf{z}_{v_n})))]
\end{equation}
represents the expected log-likelihood that we accurately predict ``false'' for an edge that does not exist in the graph. In practice, this expectation is approximated using Monte Carlo simulations.

\subsubsection{Max-Margin Loss}
Another widely used loss function for multi-relational node embedding is the \term{margin loss}:
\begin{equation}\label{eq:marginLoss}
    \mathcal{L} = \sum_{(u,\ell,v)\in E} \sum_{v_n\sim P_{n,u}(V)} \max(0, -\text{DEC}(\mathbf{z}_u, \ell, \mathbf{z}_v) + \text{DEC}(\mathbf{z}_u, \ell, \mathbf{z}_{v_n}) + \Delta).
\end{equation}
Here we compare the decoded score for a true pair against a negative sample—a strategy commonly referred to as \term{contrastive estimation}. Unlike the binary classification approach, in \Cref{eq:marginLoss} we  simply compare the direct outputs of the decoders. If the score for the ``true'' pair exceeds that of the ``negative'' pair, the loss is minimized. The term $ \Delta $ is known as the \term{margin}, so that the loss is equal to zero if the difference in scores meets or exceeds this threshold for all examples. This loss methodology is also referred to as \term{hinge loss}.

\subsection{Decoders}
The loss functions discussed in prior sections can be integrated with various decoder functions. In the \textit{RESCAL} decoder presented in \Cref{eq:RESCAL}, we associate the trainable matrix $ \mathbf{R}_\ell\in\mathbb{R}^{d\times d} $ with every label in $ L $. Nevertheless, this approach is infrequently considered due to computational and statistical costs associated with representing relations. Specifically, there exist $ O(d^2) $ parameters for each relation type in \textit{RESCAL}, with an order of magnitude more parameters to represent relations.

More popular decoders seek to utilize only $ O(d) $ parameters for each relation. 

\subsubsection{Translational Decoders}
A notable class of decoders characterizes relations as translations within the embedding space. This line of thought originated with the \textit{TransE}~\cite{Bordes2013TransE} model, which defines the decoder as:
\begin{equation*}
    \text{DEC}(\mathbf{z}_u, \ell, \mathbf{z}_v) = -\|\mathbf{z}_u + \mathbf{r}_\ell - \mathbf{z}_v\|.
\end{equation*}
In this method, each relation is represented using a $ d $-dimensional embedding. Hence, the likelihood of an edge occurring is proportional to the distance between the embedding of the head node and the tail node, following the translation of the head node according to the relation embedding.

Many works have been proposed to extend this translation idea, collectively referred to as \textit{TransX} models, having the form
\begin{equation*}
    \text{DEC}(\mathbf{z}_u, \ell, \mathbf{z}_v) = -\|g_{1,\ell}(\mathbf{z}_u) + \mathbf{r}_\ell - g_{2,\ell}(\mathbf{z}_v)\|,
\end{equation*}
where $ g_{1,\ell}, g_{2,\ell} $ stand for trainable transformations dependent on the relation $ \ell $.

\subsubsection{Multi-linear Dot Product}
Another prevalent methodology generalizes the inner product decoder from simple graphs (\Cref{eq:innerProductDec}). This approach, termed \textit{DistMult}~\cite{Yang2014DistMult}, defines the decoder as follows:
\begin{align*}
\text{DEC}(\mathbf{z}_u, \ell, \mathbf{z}_v) &= \text{Re}(\langle \mathbf{z}_u, \mathbf{r}_\ell, \overline{\mathbf{z}}_v\rangle)\\
&= \text{Re}(\sum_{i=1}^d \mathbf{z}_u[i] \times \mathbf{r}_\ell[i] \times \overline{\mathbf{z}}_v[i]),
\end{align*}
where $ \mathbf{z}_u, \mathbf{z}_v, \mathbf{r}_\ell \in \mathbb{C}^d $ are complex-valued embeddings, and $ \text{Re} $ denotes the real component of a complex vector. By taking the complex conjugate $ \overline{\mathbf{z}}_v $ of the tail embedding, this decoding approach accommodates asymmetric relations.

\subsection{An example: Het-node2vec}
As an example of technique operating on heterogeneus multi-relational graphs, expanding on the \textit{node2vec} algorithm, we can consider the \textit{Het-node2vec}~\cite{soto2021hetnode2vec} algorithm. This approach generalizes the sampling type-aware random walks  by introducing ``switching'' parameters. These variables control the way the random walk moves between
different node and edge types, providing a mechanism to incorporate the semantic information of the graph into the random walk generation process. Tuning the hyperparameters appropriately one can prioritize transitions toward specific node or edge types, allowing the algorithm to address problems where specific node/edge types are crucial for the prediction task under investigation.

\section{Graph Neural Networks}\label{sec:gnn}
The principal challenge in devising complex encoders for graph-structured data is that conventional deep learning methodologies do not apply. For instance, \term{Convolutional Neural Networks} (\term{CNNs}) are explicitly designed for grid-structured inputs such as images, while \term{Recurrent Neural Networks} (\term{RNNs}) are tailored for sequential data like text.

The concepts and architectures discussed in the following sections can be extended to encompass multi-relational graphs, similarly to the methodologies illustrated in \Cref{sec:embeddingsKGs}.

\subsection{Neural Message Passing}
The defining characteristic of a Graph Neural Network (GNN) is its use of \term{neural message passing}, that consists in the exchange of vector messages between nodes that are subsequently updated through neural networks~\cite{Gilmer2017neuralMessagePassing}. The next sections will detail the elements of this neural message passing framework.

\subsubsection{A Message Passing Framework}\label{sec:messagePassingFramework}
During each iteration of message passing in a GNN, a hidden embedding $ \mathbf{h}_u^{(\ell)} $ corresponding to each node $ u \in V $ is updated based on information aggregated from $ u $’s graph neighbourhood $ \mathcal{N}(u) $. The message-passing update illustrated in \Cref{fig:MessageAggregation} demonstrates how the messages directed toward the target node result from the aggregation of their respective neighbourhood information. Observe that the computation graph of the GNN adopts a tree structure by unfolding the neighbourhood of the target node.

\input{figs/figMessageAggregation}

The message-passing update can be mathematically conveyed as follows:
\begin{align}\label{eq:GNNupdate}
    \mathbf{h}_u^{(\ell+1)} &= \text{UPDATE}^{(\ell)}\left(\mathbf{h}_u^{(\ell)}, \text{AGGREGATE}^{(\ell)}(\{\mathbf{h}_v^{(\ell)}, \forall v \in \mathcal{N}(u)\})\right) \\
    &= \text{UPDATE}^{(\ell)}\left(\mathbf{h}_u^{(\ell)}, \mathbf{m}_{\mathcal{N}(u)}^{(\ell)}\right)
\end{align}
where UPDATE and AGGREGATE represent arbitrary differentiable functions which may also encompass complex models such as neural networks. The message $ \mathbf{m}_{\mathcal{N}(u)}^{(\ell)} $ is computed by aggregating the embeddings of $ u $'s neighbours. The subscript denotes the current iteration, often referred to as a layer within the GNN. In \Cref{fig:MessageAggregation} we portray a two-layer version of the message-passing model.

The initial embeddings (when $ \ell=0 $) are set to the input features for all nodes, $ \mathbf{h}_u^{(0)} = \mathbf{x}_u $. After $ K $ iterations of GNN message passing, the output of the final layer defines embeddings for each node, expressed as:
\begin{equation*}
    \mathbf{z}_u=\mathbf{h}_u^{(\ell)} \quad \forall v \in V.
\end{equation*}
Since the AGGREGATE function accepts sets as input, GNNs constructed in this fashion are inherently permutation equivariant.

The core intuition behind the GNN message-passing framework is that, at each iteration, every node aggregates information from its local neighbourhood, leading to progressively more informative embeddings as iterations continue. Precisely, after $ k $ iterations, each node's embedding encapsulates information pertaining to its $ k $-hop neighbourhood. The information encapsulated in the resulting node embeddings reflects both structural aspects and features present within the $ K $-hop neighbourhoods.

\subsubsection{The Basic GNN}
Until now we have discussed the GNN framework rather abstractly as a series of message-passing iterations governed by the generic functions UPDATE and AGGREGATE (\Cref{eq:GNNupdate}). 

A simplification of the original GNN models proposed by~\cite{Merkwirth2005automaticGenerationComplementaryDescriptorsMolecularGraphNetworks}\cite{scarselli2008graphNeuralNetworkModel} is expressed as:
\begin{equation}\label{eq:basicGNN}
    \mathbf{h}_u^{(\ell)} = \sigma\left(\mathbf{W}_\text{self}^{(\ell)}\mathbf{h}_u^{(\ell-1)} + \mathbf{W}_\text{neigh}^{(\ell)}\sum_{v\in\mathcal{N}(u)}\mathbf{h}_v^{(\ell-1)} + \mathbf{b}^{(\ell)}\right)
\end{equation}
where $ \mathbf{W}_\text{self}^{(\ell)}, \mathbf{W}_\text{neigh}^{(\ell)} \in \mathbb{R}^{d^{(\ell)}\times d^{(\ell-1)}} $ are learnable parameter matrices, and $ \sigma $ represents an element-wise non-linear activation function (e.g., a tanh or ReLU). Generally, the parameters $ \mathbf{W}_\text{self}, \mathbf{W}_\text{neigh} $, and $\mathbf{b}$ can be shared across GNN message-passing iterations or may be trained separately for each layer. 

One can also express the message passing rule as two steps:
\begin{align}
    \mathbf{x}_u^{(\ell)} &= \mathbf{W}_\text{self}^{(\ell)}\mathbf{h}_u^{(\ell-1)} + \mathbf{b}^{(\ell)} & \text{(feature transform)}\label{eq:featureTransform} \\ 
    \mathbf{h}_u^{(\ell)} &= \sigma\left(\mathbf{x}_u^{(\ell)} + \mathbf{W}_\text{neigh}^{(\ell)} \sum_{v\in\mathcal{N}(u)}\mathbf{h}_v^{(\ell-1)}\right) & \text{(neighbourhood aggregation)} \label{eq:neighAgg}
\end{align}

We can equivalently depict the basic GNN by defining UPDATE and AGGREGATE functions:
\begin{equation}\label{eq:aggregateBase}
    \text{AGGREGATE}(\{\mathbf{h}_v^{(\ell)}, \forall v \in \mathcal{N}(u)\}) =
    \mathbf{m}_{\mathcal{N}(u)} = \sum_{v\in\mathcal{N}(u)}\mathbf{h}_v,
\end{equation}
\begin{equation}\label{eq:updateBase}
    \text{UPDATE}(\mathbf{h}_u, \mathbf{m}_{\mathcal{N}(u)}) = \sigma(\mathbf{W}_\text{self}\mathbf{h}_u + \mathbf{W}_\text{neigh}\mathbf{m}_{\mathcal{N}(u)} + \mathbf{b}^{(\ell)}).
\end{equation}

Many GNN models can be defined succinctly using so called graph-level equations. In the case of basic GNNs, the definition in \Cref{eq:basicGNN} can be expressed as a graph-level equation as follows:

\begin{equation}\label{eq:basicGNNgraphlevel}
    \mathbf{H}^{(\ell)} = \sigma\left(\mathbf{A}\mathbf{H}^{(\ell-1)}\mathbf{W}_\text{neigh}^{(\ell)} + \mathbf{H}^{(\ell-1)}\mathbf{W}_\text{self}^{(\ell)}\right)
\end{equation}
where $\mathbf{H}^{(\ell)} \in \mathbb{R}^{|V| \times d}$ denotes the matrix of node representations at layer $\ell$ in the GNN (with each node corresponding to a row in the matrix), and $\mathbf{A}$ is the graph adjacency matrix. From here on we will be omitting the bias term $\mathbf{b}$ for notational simplicity. While this graph-level representation is not easily applicable to all GNN models—such as the attention-based models we discuss below—it is often more succinct and also highlights how many GNNs can be efficiently implemented using a small number of sparse matrix operations.


\paragraph{Self-Loops}
As a simplification of the neural message passing framework, it is customary to incorporate \term{self-loops} into the input graph while omitting the explicit update step. In this setup, the message passing is merely defined as:
\begin{equation}\label{eq:selfLoop}
    \mathbf{h}_u^{(\ell)} = \text{AGGREGATE}(\{\mathbf{h}_v^{(\ell-1)}, \forall v \in \mathcal{N}(u)\cup {u}\}),
\end{equation}
where the aggregation considers both the node's neighbours and the node itself. Herein, the update function is implicitly defined through the aggregation method. Simplifying message passing often alleviates overfitting but could severely restrict the expressiveness of the GNN, as the information from the node's neighbours cannot be differentiated from the node's inherent information.

In the case of the basic GNN adding self-loops is equivalent to sharing parameters between $\mathbf{W}_\text{self}$ and $\mathbf{W}_\text{neigh}$ matrices, so the graph-level updates is given by:
\begin{equation}\label{eq:basicGNNgraphlevelselfLoop}
    \mathbf{H}^{(\ell)} = \sigma\left(\left(\mathbf{A} + \mathbb{I}_{|V|}\right)\mathbf{H}^{(\ell-1)}\mathbf{W}^{(\ell)}\right).
\end{equation}


\subsection{AGGREGATE Generalization}
The basic GNN can be enhanced and generalized in multiple ways through variations of the AGGREGATE and UPDATE functions.

\paragraph{neighbourhood Aggregation}
The most basic aggregation operations (\Cref{eq:basicGNN}) merely compute the sum of the neighbourhood embeddings. This approach can prove unstable and highly sensitive to node degrees, with nodes possessing higher degrees expected to yield greater values in the components of their embeddings.

A potential solution is to normalize the aggregation process according to the degrees of the nodes involved. The simplest method is to compute an average instead of a sum:
\begin{equation*}
    \mathbf{m}_{\mathcal{N}(u)} = \frac{\sum_{v \in \mathcal{N}(u) \mathbf{h}_v}}{|\mathcal{N}(u)|}.
\end{equation*}
Other alternative normalization factors are successful, such as the following symmetric normalization applied by~\cite{kipf2016semi}:
\begin{equation}\label{eq:symmetricNormalization}
    \mathbf{m}_{\mathcal{N}(u)} = \sum_{v \in \mathcal{N}(u)}\frac{ \mathbf{h}_v}{\sqrt{|\mathcal{N}(u)||\mathcal{N}(v)|}}.
\end{equation}

This method reduces the influence of messages from highly connected neighbours, as their features convey less specific information.

\subsubsection{Graph Convolutional Networks (GCNs)}\label{sec:GCN}
The GCN model proposed by~\cite{kipf2016semi} combines symmetric-normalized aggregation (\Cref{eq:symmetricNormalization}) with the self-loop update approach (\Cref{eq:selfLoop}), yielding a message passing function as follows:

\begin{equation*}
    \mathbf{h}_u^{(\ell)} = \sigma\left(\mathbf{W}^{(\ell)}\sum_{v \in \mathcal{N}(u)}\frac{ \mathbf{h}_v}{\sqrt{|\mathcal{N}(u)||\mathcal{N}(v)|}}\right).
\end{equation*}

\subsubsection{Set Aggregators}
In GNNs, the neighbourhood aggregation process must maintain permutation invariance, given that node neighbourhoods possess no inherent order. We look at a pair of set aggregation methods that enhance the expressive potential of GNNs with respect to traditional methods. 

\paragraph{Set Pooling}~\cite{zaheer2017deepSets} applies a multi-layer perceptron (MLP) to each neighbour’s embedding prior to summing the outputs, followed by passing the results through another MLP. This method guarantees universal function approximation while retaining permutation invariance. 

\paragraph{Janossy Pooling}~\cite{murphy2018janossyPooling} instead of depending on permutation-invariant operations like summation, it applies a permutation-sensitive function—such as an LSTM~\cite{hochreiter1997LSTM}—to fix a canonical ordering.

\subsubsection{Neighbourhood Attention}\label{sec:neighbourhoodAttention}
Using \term{attention}~\cite{Vaswani2017attentionIsAllYouNeed} involves assigning an attention weight to each neighbour, which is utilized to amplify that neighbour's influence during the aggregation step. The first GNN model to apply this attention style, termed \term{Graph Attention Network} (\term{GAT})~\cite{Velivckovic2017graphAttentionNetworks}, uses attention weights to define a weighted sum of the neighbours:
\begin{equation*}
    \mathbf{m}_{\mathcal{N}(u)} = \sum_{v\in\mathcal{N}(u)} \alpha_{u,v}\mathbf{h}_v,
\end{equation*}
where $ \alpha_{u,v} $ denotes the attention given to neighbour $ v \in \mathcal{N}(u) $ during the aggregation for node $ u $. In the original GAT paper, the attention weights are specified as:
\begin{equation*}
    \alpha_{u,v} = \frac{\exp(\mathbf{a}^\top[\mathbf{W}\mathbf{h}_u \oplus \mathbf{W}\mathbf{h}_v])}{\sum_{v'\in\mathcal{N}(u)} \exp(\mathbf{a}^\top[\mathbf{W}\mathbf{h}_u \oplus \mathbf{W}\mathbf{h}_{v'}])}
\end{equation*}
where $ \oplus $ indicates concatenation, $ \mathbf{a} $ represents a learnable attention vector, , and $ \mathbf{W} $ denotes a trainable weight matrix.
Numerous attention models are inspired by deep learning literature. For instance, in accordance with the prevalent \term{transformer} architecture~\cite{Vaswani2017attentionIsAllYouNeed}, one can incorporate multiple attention ``heads'' in the network. In this scenario, one computes $ R $ distinct attention weights $ \alpha_{u,v,r} $ using independently parameterized attention layers. The messages aggregated using these alternative attention weights are subsequently transformed and combined during the aggregation step.

\medskip
The introduction of attention serves as a valuable strategy to enhance the representational capacity of a GNN model, particularly when prior knowledge suggests that certain neighbours may hold greater informational significance than others.

\subsection{UPDATE Generalization}

\subsubsection{Concatenation and Skip Connections} 
A crucial concern within GNNs is \term{over-smoothing}, which occurs when node-specific information becomes ``washed out''  after multiple iterations of GNN message passing. Intuitively, this is expected when the information aggregated from the node neighbours during message passing begins to dominate the updated node representations. In such circumstances, the updated node representations (i.e., the $ \mathbf{h}_u^{(\ell+1)} $) may disproportionately rely on the incoming messages aggregated from the neighbours (i.e., the $ \mathbf{m}_{\mathcal{N}(u)} $) at the expense of prior layer node representations (i.e., the $ \mathbf{h}_u^{(\ell)} $).

A straightforward technique to alleviate this involves preserving information from preceding rounds of message passing during the update step, by leveraging vector concatenation or skip connections, .

\medskip

One fundamental approach employing skip connections involves concatenation to retain node information:
\begin{equation*}
\text{UPDATE}_{\text{concat}}(\mathbf{h}_u, \mathbf{m}_{\mathcal{N}(u)}) = [\text{UPDATE}_{\text{base}}(\mathbf{h}_u, \mathbf{m}_{\mathcal{N}(u)}) \oplus \mathbf{h}_u]
\end{equation*}
where $ \mathbf{h}_u $ denotes the node’s prior representation, while $ \mathbf{m}_{\mathcal{N}(u)}$ stands for the aggregated message from neighbours, and $ \text{UPDATE}_{\text{base}} $ refers to the update function specified in \Cref{eq:updateBase}. This method, introduced in the \textit{GraphSAGE} framework~\cite{Hamilton2017inductiveRepresentationLearning}, aims to detach neighbourhood and node-specific information.

An alternative skip connection method employs linear interpolation~\cite{Pham2017columnNetworksCollectiveClassification}:
\begin{equation*}
\text{UPDATE}_{\text{interp}}(\mathbf{h}_u, \mathbf{m}_{\mathcal{N}(u)}) = \alpha_1 \circ \text{UPDATE}_{\text{base}}(\mathbf{h}_u, \mathbf{m}_{\mathcal{N}(u)}) + (1 - \alpha_1) \circ \mathbf{h}_u.
\end{equation*}
In this formulation, $ \circ $ denotes component-wise multiplication, and $ \alpha_1 \in [0,1]^d $ is a gating vector that may be learned via an auxiliary GNN~\cite{Pham2017columnNetworksCollectiveClassification}, a multi-layer perceptron (MLP), or through direct optimization.

\medskip
These techniques work to mitigate over-smoothing while enhancing the numerical stability of GNN training. They prove particularly beneficial for node classification tasks in moderately deep GNN architectures (2-5 layers) and demonstrate effectiveness in homophilic contexts, where node labels align with local neighbourhood features.

\subsubsection{Gated Updates}
Drawing inspiration from \term{Recurrent Neural Networks} (\term{RNN}s)T, the GNN message passing algorithm can be interpreted as aggregation functions that receive \term{observations} from neighbours and subsequently update the \term{hidden state} of each node. For instance, one of the earliest GNN architectures defines the update function as:
\begin{equation*}
    \mathbf{h}_u^{(\ell)} = \text{GRU}(\mathbf{h}_u^{(\ell-1)}, \mathbf{m}_{\mathcal{N}(u)}^{(\ell)}).
\end{equation*}

Here, GRU refers to the update equation of the \term{gated recurrent unit} cell~\cite{Cho2014learningPhraseRepresentations}. Alternatively, updates can be constructed based on LSTM architectures~\cite{selsam2018LearningSATSolverSingleBitSupervision}, or any update function defined within RNN frameworks.

\subsubsection{Jumping Knowledge Connections}
Thus far, we have assumed that the node representations used for downstream tasks are those produced by the final layer of the GNN.  One can enhance the quality of final node representations by leveraging the representations from each layer of the message passing process, leading to final node representations as such:
\begin{equation*}
    \mathbf{z}_u = f_\text{JK}(\mathbf{h}_u^{(0)} \oplus \mathbf{h}_u^{(1)} \oplus ... \oplus \mathbf{h}_u^{(\ell)}).
\end{equation*}
In this case, $ f_\text{JK} $ represents an arbitrary differentiable function. This strategy is referred to as \term{jumping knowledge} (\term{JK}) \term{connections}~\cite{xu2018RepresentationLearningGraphsJumpingKnowledgeNetworks}.

\subsection{Generalized Message Passing}
Until now we have focused on GNN message passing that operates at the node level. The message passing paradigm can be generalized to incorporate edge and graph-level information~\cite{battaglia2018relationalInductiveBiases} so that the equations defining a message passing step are expressed as follows:
\begin{align*}
\label\mathbf{h}^{(\ell)}_{(u,v)} &= \text{UPDATE}_\text{edge}(\mathbf{h}^{(\ell-1)}_{(u,v)}, \mathbf{h}^{(\ell-1)}_u, \mathbf{h}^{(\ell-1)}_v, \mathbf{h}^{(\ell-1)}_G)\\
\mathbf{m}_{\mathcal{N}(u)} &= \text{AGGREGATE}_\text{node}(\{\mathbf{h}^{(\ell)}_{(u,v)}, \forall v \in \mathcal{N}(u)\})\\
\mathbf{h}^{(\ell)}_u &= \text{UPDATE}_\text{node}(\mathbf{h}^{(\ell-1)}_u, \mathbf{m}_{\mathcal{N}(u)}, \mathbf{h}^{(\ell-1)}_G)\\
\mathbf{h}^{(\ell)}_G &= \text{UPDATE}_\text{graph}(\mathbf{h}^{(\ell-1)}_G, \{\mathbf{h}^{(\ell)}_u, \forall u \in V\}, \{\mathbf{h}^{(\ell)}_{(u,v)}, \forall (u,v) \in E\}).
\end{align*}

The key innovation within this generalized message passing framework is the generation of hidden embeddings $ \mathbf{h}^{(\ell)}_{(u,v)} $ for each edge in the graph, alongside an embedding $ \mathbf{h}^{(\ell)}_G $ representing the entire graph. 
The update is brought out in the order listed above. Each of the individual update and aggregation operations within this generalized message-passing framework can be implemented utilizing the techniques discussed within this section.