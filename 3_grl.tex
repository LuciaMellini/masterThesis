\chapter{Graph representation learning}\label{grl}

\section{Graph learning problems}
Machine learning is inherently a problem-driven discipline. We seek to build models that can learn from data in order to solve particular tasks, and machine learning models are often categorized according to the type of task they seek to solve: supervised or unsupervised. Learning with graphs is no different, but the usual categories of supervised and unsupervised are not necessarily the most informative or useful when it comes to graphs. In this section we provide a brief overview of the most important and well-studied machine learning tasks on graph data. As we will see machine learning problems on graphs often blur the boundaries between the traditional machine learning categories.

\subsection{Node classification}
Given a graph $G=(V,E,L)$ in \term{node classification} the goal is to predict the label associated with each node $v\in V$, when we only given the true labels on a \term{training set} of nodes $V_\text{train}\subset V$. Often one assumes that we only have label information for a small subset of nodes, but there aare instances that involve many labeled nodes, like classifying the function of proteins in the interactomes of different species~\cite{Hamilton2017inductive}.
Even though node classification appears to be a straightforward variation of standard supervised learning, there are important differences. The nodes in a graph, in fact, are not independent and identically distributed since we are modelling an interconnected set of nodes. The key insight behind many of the most successful node classification approaches is to explicitly leverage the connections between nodes. One particularly popular idea is to exploit \term{homophily}, which is the tendency for nodes to share attributes with their neighbors in the graph~\cite{Mcpherson2001homophilyInSocialNw}. For example, people tend to form friendships with others who share the same interests or demographics. Based on the notion of homophily we can build machine learning models that try to assign similar labels to neighboring nodes in a graph~\cite{Zhou2003LearningLocalGlobalConsistency}. Beyond homophily there are also concepts such as structural equivalence~\cite{Donnat2017GraphWaveletsStructuralRoleSimilarityComplexNws}, which is the idea that nodes with similar local neighborhood structures will have similar labels, as well as \term{heterophily}, which presumes that nodes will be preferentially connected to nodes with different labels. When we build node classification models we want to exploit these concepts and model the relationships between nodes, rather than simply treating nodes as independent datapoints.

\subsection{Link prediction}
The standard setup for \term{link prediction} is that we are given a set of nodes $V$ and an incomplete set of edges between these nodes $E_\text{train} \subset E$. Our goal is to use this partial information to infer the missing edges $E \setminus E_\text{train}$. This is one of the most popular machine learning tasks with graph data, and has countless applications, like predicting drug side-effects~\cite{Zitnik2018ModelingPolypharmacySideEffectsGCN}. In complex multi-relational graph datasets, such as BKGs that encode hundreds of different biological interactions, link prediction can require complex reasoning and inference strategies~\cite{Nickel2015ReviewRelationalMLKG}. Ofter link prediction requires inductive biases that are specific to the graph domain.

\subsection{Clustering and community detection}
Both node classification and link prediction require inferring missing information about graph data, and in many ways, those two tasks are the graph analogues of supervised learning. Community detection, on the other hand, is the graph analogue of unsupervised clustering. The general intuition underlying the task of clustering is that the graph exhibits a community structure, where nodes are much more likely to form edges with nodes that belong to the same community. The challenge of community detection is to infer latent community structures given only the input graph $G = (V, E)$. The many real-world applications of community detection include uncovering functional modules in genetic interaction networks~\cite{Agrawal2018LargeScaleAnalysisDiseasePathwaysHumanInteractome}.

\subsection{Why graph representation learning?}
To solve the problems above there exist various ``traditional'' methods~\cite{Hamilton2020GraphRL}, such as the following:
\begin{itemize}
    \item graph statistics and kernel methods to extract feature information for classification tasks,
    \item neighbourhood overlap detection to provide heuristics for link prediction,
    \item spectral clustering and graph Laplacians to cluster nodes into communities.
\end{itemize}

However, these approaches are limited due to the fact that they require careful, hand-engineered statistics and measures. These hand-engineered features cannot adapt through a learning process, and designing these features can be a time-consuming and expensive process. The following chapters in this book introduce alternative approach to learning over graphs: graph representation learning. Instead of extracting hand-engineered features, we will seek to learn representations that encode structural information about the graph.


\section{Node embeddings}
The goal of these methods is to encode nodes as low-dimensional vectors or \term{embeddings} that summarize their graph position and the structure of their local graph neighborhood. In other words, we want to project nodes into a latent space, where geometric relations in this latent space correspond to relationships in the original graph~\cite{Hoff2002latentSpaceApproachesSocialNetworkAnalysis}. In Figure~\ref{fig:nodeEmbedding} we illustrate this problem; our goal is to learn an encoder that maps nodes to a low-dimensional embedding space. These embeddings are optimized so that distances in the embedding space reflect the relative positions of the nodes in the original graph. 
\input{figs/figNodeEmbedding}

\subsection{An Encoder-Decoder framework}
We organize our discussion of node embeddings based upon the framework of encoding and decoding graphs, closely following the perspective in~\cite{Hamilton2017RLonGraphs}. In the encoder-decoder framework, we view the graph representation learning problem as involving two key operations. 
\begin{description}
    \item[Encoder model] maps each node in the graph into a low-dimensional vector or embedding,
    \item[Decoder model] takes the low-dimensional node embeddings and uses them to reconstruct
information about each node's neighborhood in the original graph.
\end{description}
This idea is summarized in Figure~\ref{fig:encDecFramework}.
In this section we focus on \term{shallow} embedding methods. The encoder can be generalized beyond the shallow embedding approach, for instance using node features or local graph structure around each node as input to generate the embedding, These generalized encoder architectures, often called graph neural networks (GNNs), are the focus of the next section.

\input{figs/figEncDecFramework}

\subsubsection{Encoder}
Formally, an encoder is a function that maps nodes to vector embeddings, so in the simplest case the encoder has the following signature:
\begin{equation}
    \text{ENC}: \mathcal{V} \to \mathbb{R}^d
\end{equation}
When talking about \term{shallow} node embeddings, the encoder function is simply an embedding lookup base on the node id. So we have that
\begin{equation}
    \text{ENC}(v) = \mathbf{z}[v]
\end{equation}
where $\mathbf{z}\in \mathbb{R}^{\mathcal{V}\times d}$ is a matrix containing the embedding vectors for all the nodes and $\mathbb{Z}[v]$ denotes the row of the matrix corresponding to node $v$.

\subsubsection{Limitations of shallow embeddings}

\subsection{Multi-relational data and knowledge graphs}

\section{Graph Neural Networks}

