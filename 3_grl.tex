\chapter{Graph representation learning}\label{grl}

\section{Graph learning problems}
Machine learning is inherently a problem-driven discipline. We seek to build models that can learn from data in order to solve particular tasks, and machine learning models are often categorized according to the type of task they seek to solve: supervised or unsupervised. Learning with graphs is no different, but the usual categories of supervised and unsupervised are not necessarily the most informative or useful when it comes to graphs. In this section we provide a brief overview of the most important and well-studied machine learning tasks on graph data. As we will see machine learning problems on graphs often blur the boundaries between the traditional machine learning categories.

\subsection{Node classification}
Given a graph $G=(V,E,L)$ in \term{node classification} the goal is to predict the label associated with each node $v\in V$, when we only given the true labels on a \term{training set} of nodes $V_\text{train}\subset V$. Often one assumes that we only have label information for a small subset of nodes, but there aare instances that involve many labeled nodes, like classifying the function of proteins in the interactomes of different species~\cite{Hamilton2017inductive}.
Even though node classification appears to be a straightforward variation of standard supervised learning, there are important differences. The nodes in a graph, in fact, are not independent and identically distributed since we are modelling an interconnected set of nodes. The key insight behind many of the most successful node classification approaches is to explicitly leverage the connections between nodes. One particularly popular idea is to exploit \term{homophily}, which is the tendency for nodes to share attributes with their neighbors in the graph~\cite{Mcpherson2001homophilyInSocialNw}. For example, people tend to form friendships with others who share the same interests or demographics. Based on the notion of homophily we can build machine learning models that try to assign similar labels to neighboring nodes in a graph~\cite{Zhou2003LearningLocalGlobalConsistency}. Beyond homophily there are also concepts such as structural equivalence~\cite{Donnat2017GraphWaveletsStructuralRoleSimilarityComplexNws}, which is the idea that nodes with similar local neighborhood structures will have similar labels, as well as \term{heterophily}, which presumes that nodes will be preferentially connected to nodes with different labels. When we build node classification models we want to exploit these concepts and model the relationships between nodes, rather than simply treating nodes as independent datapoints.

\subsection{Link prediction}
The standard setup for \term{link prediction} is that we are given a set of nodes $V$ and an incomplete set of edges between these nodes $E_\text{train} \subset E$. Our goal is to use this partial information to infer the missing edges $E \setminus E_\text{train}$. This is one of the most popular machine learning tasks with graph data, and has countless applications, like predicting drug side-effects~\cite{Zitnik2018ModelingPolypharmacySideEffectsGCN}. In complex multi-relational graph datasets, such as BKGs that encode hundreds of different biological interactions, link prediction can require complex reasoning and inference strategies~\cite{Nickel2015ReviewRelationalMLKG}. Ofter link prediction requires inductive biases that are specific to the graph domain.

\subsection{Clustering and community detection}
Both node classification and link prediction require inferring missing information about graph data, and in many ways, those two tasks are the graph analogues of supervised learning. Community detection, on the other hand, is the graph analogue of unsupervised clustering. The general intuition underlying the task of clustering is that the graph exhibits a community structure, where nodes are much more likely to form edges with nodes that belong to the same community. The challenge of community detection is to infer latent community structures given only the input graph $G = (V, E)$. The many real-world applications of community detection include uncovering functional modules in genetic interaction networks~\cite{Agrawal2018LargeScaleAnalysisDiseasePathwaysHumanInteractome}.

\subsection{Why graph representation learning?}
To solve the problems above there exist various ``traditional'' methods~\cite{Hamilton2020GraphRL}, such as the following:
\begin{itemize}
    \item graph statistics and kernel methods to extract feature information for classification tasks,
    \item neighbourhood overlap detection to provide heuristics for link prediction,
    \item spectral clustering and graph Laplacians to cluster nodes into communities.
\end{itemize}

However, these approaches are limited due to the fact that they require careful, hand-engineered statistics and measures. These hand-engineered features cannot adapt through a learning process, and designing these features can be a time-consuming and expensive process. The following chapters in this book introduce alternative approach to learning over graphs: graph representation learning. Instead of extracting hand-engineered features, we will seek to learn representations that encode structural information about the graph.

\subsubsection{Limitations of shallow embeddings}

\subsection{Multi-relational data and knowledge graphs}

\section{Graph Neural Networks}

