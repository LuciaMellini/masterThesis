\chapter{Graph representation learning}\label{grl}

\section{Graph learning problems}
Machine learning is inherently a problem-driven discipline. We seek to build models that can learn from data in order to solve particular tasks, and machine learning models are often categorized according to the type of task they seek to solve: supervised or unsupervised. Learning with graphs is no different, but the usual categories of supervised and unsupervised are not necessarily the most informative or useful when it comes to graphs. In this section we provide a brief overview of the most important and well-studied machine learning tasks on graph data. As we will see machine learning problems on graphs often blur the boundaries between the traditional machine learning categories.

\subsection{Node classification}
Given a graph $G=(V,E,L)$ in \term{node classification} the goal is to predict the label associated with each node $v\in V$, when we only given the true labels on a \term{training set} of nodes $V_\text{train}\subset V$. Often one assumes that we only have label information for a small subset of nodes, but there aare instances that involve many labeled nodes, like classifying the function of proteins in the interactomes of different species~\cite{Hamilton2017inductive}.
Even though node classification appears to be a straightforward variation of standard supervised learning, there are important differences. The nodes in a graph, in fact, are not independent and identically distributed since we are modelling an interconnected set of nodes. The key insight behind many of the most successful node classification approaches is to explicitly leverage the connections between nodes. One particularly popular idea is to exploit \term{homophily}, which is the tendency for nodes to share attributes with their neighbors in the graph~\cite{Mcpherson2001homophilyInSocialNw}. For example, people tend to form friendships with others who share the same interests or demographics. Based on the notion of homophily we can build machine learning models that try to assign similar labels to neighboring nodes in a graph~\cite{Zhou2003LearningLocalGlobalConsistency}. Beyond homophily there are also concepts such as structural equivalence~\cite{Donnat2017GraphWaveletsStructuralRoleSimilarityComplexNws}, which is the idea that nodes with similar local neighborhood structures will have similar labels, as well as \term{heterophily}, which presumes that nodes will be preferentially connected to nodes with different labels. When we build node classification models we want to exploit these concepts and model the relationships between nodes, rather than simply treating nodes as independent datapoints.

\subsection{Link prediction}
The standard setup for \term{link prediction} is that we are given a set of nodes $V$ and an incomplete set of edges between these nodes $E_\text{train} \subset E$. Our goal is to use this partial information to infer the missing edges $E \setminus E_\text{train}$. This is one of the most popular machine learning tasks with graph data, and has countless applications, like predicting drug side-effects~\cite{Zitnik2018ModelingPolypharmacySideEffectsGCN}. In complex multi-relational graph datasets, such as BKGs that encode hundreds of different biological interactions, link prediction can require complex reasoning and inference strategies~\cite{Nickel2015ReviewRelationalMLKG}. Ofter link prediction requires inductive biases that are specific to the graph domain.

\subsection{Clustering and community detection}
Both node classification and link prediction require inferring missing information about graph data, and in many ways, those two tasks are the graph analogues of supervised learning. Community detection, on the other hand, is the graph analogue of unsupervised clustering. The general intuition underlying the task of clustering is that the graph exhibits a community structure, where nodes are much more likely to form edges with nodes that belong to the same community. The challenge of community detection is to infer latent community structures given only the input graph $G = (V, E)$. The many real-world applications of community detection include uncovering functional modules in genetic interaction networks~\cite{Agrawal2018LargeScaleAnalysisDiseasePathwaysHumanInteractome}.

\subsection{Why graph representation learning?}
To solve the problems above there exist various ``traditional'' methods~\cite{Hamilton2020GraphRL}, such as the following:
\begin{itemize}
    \item graph statistics and kernel methods to extract feature information for classification tasks,
    \item neighbourhood overlap detection to provide heuristics for link prediction,
    \item spectral clustering and graph Laplacians to cluster nodes into communities.
\end{itemize}

However, these approaches are limited due to the fact that they require careful, hand-engineered statistics and measures. These hand-engineered features cannot adapt through a learning process, and designing these features can be a time-consuming and expensive process. The following chapters in this book introduce alternative approach to learning over graphs: graph representation learning. Instead of extracting hand-engineered features, we will seek to learn representations that encode structural information about the graph.

\section{Node embeddings}
The goal of these methods is to encode nodes as low-dimensional vectors or \term{embeddings} that summarize their graph position and the structure of their local graph neighborhood. In other words, we want to project nodes into a latent space, where geometric relations in this latent space correspond to relationships in the original graph~\cite{Hoff2002latentSpaceApproachesSocialNetworkAnalysis}. In Figure~\ref{fig:nodeEmbedding} we illustrate this problem; our goal is to learn an encoder that maps nodes to a low-dimensional embedding space. These embeddings are optimized so that distances in the embedding space reflect the relative positions of the nodes in the original graph. 
\input{figs/figNodeEmbedding}
We introducing the encoder-decoder framework as a way of explaining the workings of node embeddings. We then illustrate some node embedding methods; we especially focus on random walk approaches seen the techniques used s baseline during our experiments.

\subsection{An Encoder-Decoder framework}
We organize our discussion of node embeddings based upon the framework of encoding and decoding graphs, closely following the perspective in~\cite{Hamilton2020GraphRL}. In the encoder-decoder framework, we view the graph representation learning problem as involving two key operations. 
\begin{description}
    \item[Encoder] maps each node in the graph into a low-dimensional vector or embedding,
    \item[Decoder] takes the low-dimensional node embeddings and uses them to reconstruct information about each node's neighborhood in the original graph.
\end{description}
This idea is summarized in Figure~\ref{fig:encDecFramework}.
In this section we focus on \term{shallow} embedding methods. The encoder can be generalized beyond the shallow embedding approach, for instance using node features or local graph structure around each node as input to generate the embedding, These generalized encoder architectures, often called graph neural networks (GNNs), are the focus of the next section.

\input{figs/figEncDecFramework}

\subsubsection{Encoder}
Formally, an \term{encoder} is a function that maps nodes to vector embeddings, so in the simplest case the encoder has the following signature:
\begin{equation*}
    \text{ENC}: \mathcal{V} \to \mathbb{R}^d.
\end{equation*}
When talking about \term{shallow} node embeddings, the encoder function is simply an embedding lookup base on the node id. So for a node $v\in \mathcal{V}$ we have that
\begin{equation}\label{eq:encoderLookup}
    \text{ENC}(v) = \mathbf{z}[v]
\end{equation}
where $\mathbf{z}\in \mathbb{R}^{\mathcal{V}\times d}$ is a matrix containing the embedding vectors for all the nodes and $\mathbb{Z}[v]$ denotes the row of the matrix corresponding to node $v$.\

\subsubsection{Decoder}\label{sec:decoder}
The role of the \term{decoder} is to reconstruct certain graph statistics from the node embeddings that are generated by the encoder. For example, given the node embedding $\mathbf{z}_v$ of a node $v$ the decoder might attempt to predict its neighbourhood $\mathcal{N}(v)$.
The most common definition is a \term{pairwise} decoder, with the following signature:
\begin{equation*}
    \text{DEC}: \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R}^+.
\end{equation*}
Pairwise decoders are used to estimate relationships or similarities between pairs of nodes. For instance, a pairwise decoder can be used to predict if two nodes are neighbours in a graph.

Applying the pairwise decoder to a pair of embeddings $(\mathbf{z}_u, \mathbf{z}_v)$ results in the \term{reconstruction} of the relationship between nodes $u$ and $v$. The goal is to optimize the encoder and decoder as to minimize the reconstruction loss so that
\begin{equation}\label{eq:reconstruction}
    \text{DEC}(\text{ENC}(u), \text{ENC}(v)) = \text{DEC}(\mathbf{z}_u, \mathbf{z}_v) \approx \textbf{S}(u,v)
\end{equation}
where $\textbf{S}$ is a graph similarity measure between nodes. In the case of predicting edges in a graph the similarity function of a pair of nodes could be equivalent to the respective entry in the graph's adjacency matrix.

\subsubsection{Optimizing the Encoder-Decoder model}
To achieve the reconstruction objective (Equation~\eqref{eq:reconstruction}), the standard practice is to minimize an empirical reconstruction loss $\mathcal{L}$ over a set of training node pairs $\mathcal{D}$:
\begin{equation}\label{eq:loss}
    \mathcal{L} = \sum_{(u,v)\in\mathcal{D}} \ell(\text{DEC}(\mathbf{z}_u, \mathbf{z}_v), \textbf{S}[u,v])
\end{equation}
where $\ell: \mathbb{R}\times\mathbb{R}\to\mathbb{R}$ is a loss function measuring the discrepancy between the decoded similarity values $\text{DEC}(\mathbf{z}_u, \mathbf{z}_v)$ and the true values $\textbf{S}[u,v]$. The choice of the loss function $\ell$ depends on the used decoder and the chosen similarity measure $\textbf{S}$. Simple loss functions are mean-squared error for regression tasks, or cross entropy for classification. Most approaches use stochastic gradient descent to minimize the loss in Equation~\eqref{eq:loss}~\cite{Robbins1951stochasticApproximation}, but one can used more specialized optimization methods, for example based on matrix factorization, that have a close theoretical connections to spectral clustering. 

In the following sections we describe a few representative node embedding methods. We will begin with a discussion of node embedding methods that are motivated by matrix factorization approaches based on random walks. Following this, we will discuss more recent methods based on random walks. These approaches were initially motivated by inspirations from natural language processing, but they also share close theoretical ties to spectral graph theory. We focus on these more recent methods seen that we have used them to have a baseline measure of performance in our experiments. 

\subsection{Factorization-based approaches}
One way of viewing the encoder-decoder idea is from the perspective of matrix factorization. Indeed, the challenge of decoding local neighborhood structure from a node's embedding is closely related to reconstructing entries in the graph adjacency matrix. More generally, we can view this task as using matrix factorization to learn a low-dimensional approximation of a node-node similarity matrix S, where $\mathbb{S}$ generalizes the adjacency matrix and captures some user-defined notion of node-node similarity (as discussed in Section~\ref{sec:decoder})~\cite{Belkin2001laplacianEigenmapsSpectralClusteringTechniquesEmbeddingClustering}\cite{Kruskal1964MultidimensionalScalingOptimizingGoodnessFitNonmetricHypothesis}.
\subsubsection{Laplacian eigenmaps}
One of the earliest factorization-based approaches is the \term{Laplacian eigenmaps} (\term{LE}) technique, which builds upon spectral clustering~\cite{Belkin2001laplacianEigenmapsSpectralClusteringTechniquesEmbeddingClustering}. In this approach, we define the decoder based on the $L_2$-distance between the node embeddings:
\begin{equation*}
    \text{DEC}(\mathbf{z}_u, \mathbf{z}_v) = \|\mathbf{z}_u - \mathbf{z}_v\|_2^2.
\end{equation*}
The loss function then weighs pairs of nodes according to their similarity in the graph:
\begin{equation}\label{eq:laplacianEigenmaploss}
    \mathcal{L} = \sum_{(u,v)\in\mathcal{D}} \text{DEC}(\mathbf{z}_u, \mathbf{z}_v) \cdot \textbf{S}[u,v].
\end{equation}

The intuition behind this approach is that Equation~\eqref{eq:laplacianEigenmaploss} penalizes the model when very similar nodes have embeddings that are far apart. If $\mathbb{S}$ is constructed so that it satisfies the properties of a Laplacian matrix and if the embeddings $z_u$ are $d$-dimensional, then the node embeddings that minimize the loss in Equation ~\eqref{eq:laplacianEigenmaploss} are the $d $smallest eigenvectors of the Laplacian (excluding the eigenvector of all ones).

\subsubsection{Inner-product methods}
More recent work geerally employs an inner-product based decoder defines as follows:
\begin{equation*}
    \text{DEC}(\mathbf{z}_u, \mathbf{z}_v) = \mathbf{z}_u^T\mathbf{z}_v.
\end{equation*}
Here, we assume that the similarity between two nodes is proportional to the dot product of their embeddings. These embedding styles~\cite{Ahmed2013distributedLargeScaleNaturalGraphFactorization}\cite{Cao2015grarep}\cite{Ou2016asymmetricTransitivityPreservingGraphEmbedding} combine the inner-product decoder with the following mean-squared error:
\begin{equation*}
    \mathcal{L} = \sum_{(u,v)\in\mathcal{D}} \|\text{DEC}(\mathbf{z}_u, \mathbf{z}_v) - \textbf{S}[u,v]\|_2^2..
\end{equation*}
These methods are referred to as matrix-factorization approaches, since their loss functions can be minimized using factorization algorithms, such as the singular value decomposition (SVD). Indeed, by stacking the node embeddings $\mathbf{z}_u \in \mathbb{R}^d$ into a matrix $\mathbf{Z} \in \mathbb{R}^{|\mathcal{V}|\times d}$ the reconstruction objective for these approaches can be written as
\begin{equation*}
    \mathcal{L} \approx \|\mathbf{Z}\mathbf{Z}^\top - \textbf{S}\|_2^2,
\end{equation*}
which corresponds to a low-dimensional factorization of the node-node similarity matrix $\textbf{S}$. Intuitively, the goal of these methods is to learn embeddings for each node such that the inner product between the learned embedding vectors approximates some deterministic measure of node similarity.


\subsection{Random walk embeddings}
These approaches adapt the inner-product approach to use \term{stochastic} measures of neighborhood overlap. The key intuition in these approaches is that node embeddings are optimized so that two nodes have similar embeddings if they tend to co-occur on short random walks over the graph.

\subsubsection{DeepWalk \& node2vec}
Similar to the matrix factorization approaches described above, \textit{DeepWalk} and \textit{node2vec} use a shallow embedding approach and an inner-product decoder.

The key distinction in these methods is in how they define the notions of node similarity and neighborhood reconstruction. Instead of directly reconstructing the adjacency matrix $\mathbf{A}$---or some deterministic function of $\mathbf{A}$---these approaches optimize embeddings to encode the statistics of random walks. Mathematically, the goal is to learn embeddings so that the following (roughly) holds:
\begin{equation}\label{eq:randomWalkdec}
    \text{DEC}(\mathbf{z}_u, \mathbf{z}_v) = \frac{e^{\mathbf{z}_u^\top\mathbf{z}_v}}{\sum_{v_k\in\mathcal{V}} e^{\mathbf{z}_u^\top\mathbf{z}_k}} \approx p_{\mathcal{G},T}(v|u),
\end{equation}
where $p_{\mathcal{G},T}(v|u)$, approximated with a softmax function, is the probability of visiting $v$ on a length-$T$ random walk starting at $u$, with $T$ usually defined to be in the range $T\in\{2,...,10\}$. Again, a key difference between this equation and the factorization-based approaches is that the similarity measure is both stochastic and asymmetric.
To train random walk embeddings, the general strategy is to use the decoder from Equation~\eqref{eq:randomWalkdec} and minimize the following cross-entropy loss:
\begin{equation}\label{eq:randomWalkloss}
    \mathcal{L} = \sum_{(u,v)\in\mathcal{D}} -\log(\text{DEC}(\mathbf{z}_u, \mathbf{z}_v)).
\end{equation}

Here, we use $\mathcal{D}$ to denote the training set of random walks, which is generated by sampling random walks starting from each node. For example, we can assume that $N$ pairs of co-occurring nodes for each node $u$ are sampled from the distribution $(u,v) \sim p_{\mathcal{G},T}(v|u)$.

Unfortunately, however, naively evaluating the loss in Equation~\eqref{eq:randomWalkloss} can be computationally expensive. Indeed, evaluating the denominator in Equation~\eqref{eq:randomWalkdec} alone has time complexity $\mathcal{O}(|\mathcal{V}|)$, which makes the overall time complexity of evaluating the loss function $\mathcal{O}(|\mathcal{D}||\mathcal{V}|)$. There are different strategies to overcome this computational challenge, and this is one of the essential differences between the original \textit{DeepWalk} and \textit{node2vec} algorithms. \textit{DeepWalk} employs a \term{hierarchical softmax}\footnote{Hierarchical softmax approximates softmax efficiently by structuring output classes (graph nodes) into a binary tree, reducing complexity from $O(N)$ to $O(\log(N))$.Probabilities are computed along the root-to-leaf path using logistic sigmoid functions, and a Huffman tree optimizes efficiency by placing frequent classes closer to the root.} to approximate Equation~\eqref{eq:randomWalkdec}~\cite{Perozzi2014DeepWalk}. On the other hand, \textit{node2vec} employs a noise contrastive approach to approximate Equation~\eqref{eq:randomWalkloss}, where the normalizing factor is approximated using negative samples in the following way~\cite{Grover2016node2vec}:
\begin{equation}\label{eq:node2vecLoss}
    \mathcal{L} = \sum_{(u,v)\in\mathcal{D}} -\log(\sigma(\mathbf{z}_u^\top\mathbf{z}_v)) - \gamma\mathbb{E}_{v_n\sim P_n(\mathcal{V})}[\log(-\sigma(\mathbf{z}_u^\top\mathbf{z}_{v_n}))].
\end{equation}
Here, we use $\sigma$ to denote the logistic function, $P_n(\mathcal{V})$ to denote a distribution over the set of nodes $\mathcal{V}$, and we assume that $\gamma > 0$ is a hyperparameter. In practice $P_n(\mathcal{V})$ is often defined to be a uniform distribution, and the expectation is approximated using Monte Carlo sampling.

The \textit{node2vec} approach also distinguishes itself from the earlier \textit{DeepWalk} algorithm by allowing for a more flexible definition of random walks. In particular, whereas \textit{DeepWalk} simply employs uniformly random walks to define $p_{\mathcal{G},T}(v|u)$, the \textit{node2vec} approach introduces hyperparameters that allow the random walk probabilities to smoothly interpolate between walks that are more akin to breadth-first search or depth-first search over the graph.

\subsubsection{Large-scale information network embeddings (LINE)}
The \textit{LINE} approach~\cite{Tang2015line} does not explicitly leverage random walk strategies, but shares conceptual motivations with \textit{DeepWalk} and \textit{node2vec}. The basic idea in LINE is to combine two encoder-decoder objectives. The first objective aims to encode first-order adjacency information and uses the following decoder:
\begin{equation*}
    \text{DEC}(\mathbf{z}_u, \mathbf{z}_v) = \frac{1}{1 + e^{-\mathbf{z}_u^\top\mathbf{z}_v}},
\end{equation*}
with an adjacency-based similarity measure (i.e., $\textbf{S}[u,v] = \mathbf{A}[u,v]$). The second objective is more similar to the random walk approaches. It is the same decoder as Equation~\eqref{eq:randomWalkdec}, but it is trained using the KL-divergence\footnote{Kullback–Leibler (KL) divergence is a type of statistical distance: a measure of how much a model probability distribution $Q$ is different from a true probability distribution $P$. Mathematically, it is defined as $D_{\text{KL}}(P\parallel Q)=\sum_{x\in {\mathcal{X}}}P(x)\ \log \left({\frac{\ P(x)\ }{Q(x)}}\right)$.} to encode two-hop adjacency information (i.e., the information in $\mathbf{A}^2$). Thus, LINE is conceptually related to \textit{node2vec} and \textit{DeepWalk}, however, instead of sampling random walks, it explicitly reconstructs first- and second-order neighborhood information.

%One beneﬁt of the random walk approach is that it can be extended and modiﬁed by biasing or modifying the random walks.

\subsection{Limitations of shallow embeddings}
In shallow embedding approaches the encoder model that maps nodes to embeddings is simply an embedding lookup (Equation~\ref{eq:encoderLookup}), which trains a unique embedding for each node in th graph. These approaches suffer from some important drawbacks:
\begin{itemize}
    \item They do not share any parameters between nodes in the encoder, since the encoder directly optimizes a unique embedding vector for each nodes. This lack of parameter sharing is both statistically and computationally inefficient. From a statistical perspective, parameter sharing can improve the efficient of learning and also act as a powerful form of regularization. From the computational point of view, the lack of parameter sharing results in the number of parameters growing as $O(\mathcal{V})$, which can be intractable in massive graphs.
    \item The do not leverage node features in the encoder. Many graph datasets have rich feature information, which could potentially be informative in the encoding process.
    \item These methods are inherently \term{transductive}, meaning they can only generate embeddings for nodes that wew present during the training phase. Generating embeddings for new nodes observed after the training phase is not possible unless additional optimizations are performed to learn the e,beddings for these nodes. This restriction prevents shallow embedding methods from being used on \term{inductive0} applications, which involve generalizing to unseen nodes after training.
\end{itemize}
To alleviate these limitations, shallow encoders can be replaced with more sophisticated ones that depend more generally on the structure and attributes of the graph. We discuss the most popular paradigm to define such encoders, graph neural networks (GNNs), in Section~\ref{sec:gnn}.


\section{Graph Neural Networks}\label{sec:gnn}

